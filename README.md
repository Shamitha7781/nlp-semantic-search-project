# nlp-semantic-search-project
NLP project comparing Word2Vec, BERT, and Sentence-BERT for semantic search on large-scale unstructured text data. Focuses on text preprocessing, embedding generation, cosine similarity, and evaluating contextual relevance beyond keyword-based search.
ğŸ” Problem Statement

Organizations often deal with large volumes of unstructured text such as reviews, feedback, or open-ended responses. Keyword-based search systems struggle to retrieve relevant content when phrasing differs, intent is implicit, or sentiment is nuanced.

This project evaluates semantic search approaches using Word2Vec, BERT, and Sentence-BERT to determine which model most effectively retrieves contextually relevant documents based on meaning rather than exact keywords.
ğŸ—‚ï¸ Dataset

IMDb Large Movie Review Dataset

50,000 labeled text reviews (positive & negative)

Long, unstructured, sentiment-heavy text â€” ideal for semantic search evaluation

ğŸ“Œ The dataset is publicly available and not included in this repository.
âš™ï¸ Project Workflow

Data Loading & Cleaning

HTML tag removal

Lowercasing

Tokenization

Stopword removal (Word2Vec only)

Lemmatization

Text Preparation

Sentence padding & truncation (for transformer models)

Train/test split (80/20)

Model Implementation

Word2Vec

BERT

Sentence-BERT

Semantic Search Evaluation

Query embedding

Cosine similarity

Top-K document retrieval

ğŸ§  Models Implemented
ğŸ”¹ Word2Vec

Learns word-level embeddings using local context windows

Sentence embeddings generated by averaging word vectors

Fast but limited in capturing sentence-level meaning

ğŸ”¹ BERT

Transformer-based model with bidirectional context understanding

Fine-tuned for sentiment classification

Embeddings adapted for semantic similarity

ğŸ”¹ Sentence-BERT

Optimized specifically for semantic similarity tasks

Generates fixed-length sentence embeddings

Best overall performance for semantic search

ğŸ“Š Evaluation & Results

Word2Vec: Captures basic semantic similarity but struggles with context

BERT: Strong contextual understanding but slower for large-scale retrieval

Sentence-BERT: Most accurate and efficient for semantic search tasks

ğŸ“Œ Sentence-BERT consistently retrieved the most contextually relevant documents for a given query.

ğŸ’¡ Example Use Cases

Semantic document retrieval

Feedback and review analysis

Knowledge base search

NLP-powered recommendation systems

Search optimization beyond keyword matching

ğŸš€ Technologies Used

Python

NLTK

Gensim

PyTorch

Hugging Face Transformers

Sentence-Transformers

Pandas / NumPy

Scikit-learn

ğŸ“„ How to Run the Project

Clone the repository

Install dependencies:

pip install -r requirements.txt


Download and extract the IMDb dataset

Open and run ML_Preprocessing_A.ipynb step by step

ğŸ¯ Why This Project Matters

Demonstrates practical NLP application, not just theory

Compares classical vs transformer-based embeddings

Shows full ML lifecycle: preprocessing â†’ modeling â†’ evaluation

Transferable to real-world semantic search problems

ğŸ“Œ Future Improvements

Add FAISS indexing for scalable retrieval

Experiment with RoBERTa or domain-specific transformers

Implement quantitative semantic search metrics

Deploy as an API or interactive demo

ğŸ“¬ Author
Shamitha Preethi Miriyala
Data Analyst | NLP & Machine Learning
